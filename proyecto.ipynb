{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeLtiuPVps92DF+Lg8Q7Pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonio1134/IA-ComputerVision/blob/main/proyecto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ceV6ukeEKQUL"
      },
      "outputs": [],
      "source": [
        "# Actualizar el sistema e instalar Python 3.8 y su entorno virtual\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8 python3.8-dev python3.8-venv -y\n",
        "\n",
        "# Crear y activar el entorno virtual\n",
        "!python3.8 -m venv tfjs-venv\n",
        "!source tfjs-venv/bin/activate\n",
        "\n",
        "# Actualizar pip e instalar TensorFlow y TensorFlow.js\n",
        "!tfjs-venv/bin/pip install --upgrade pip\n",
        "!tfjs-venv/bin/pip install tensorflow==2.8 tensorflowjs tensorflow-decision-forests tensorflow_datasets\n",
        "\n",
        "# Configurar entorno Keras\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libhdf5-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6tm0qW5pKVzf",
        "outputId": "db36e700-7cf6-47c1-af8e-a41141102c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libhdf5-dev is already the newest version (1.10.7+repack-4ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Función para crear carpetas si no existen\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "# subir carpeta al entorno de colab\n",
        "print(\"por favor, sube tu carpeta de imágenes (organizada por clases en subcarpetas):\")\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()  # sube archivo\n",
        "uploaded_filename = list(uploaded.keys())[0]  # Obtiene el nombre del archivo subido\n",
        "\n",
        "# Descompre el archivo subido\n",
        "dataset_path = '/content/dataset'\n",
        "with zipfile.ZipFile(uploaded_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_path)\n",
        "print(f\"Carpeta descomprimida en: {dataset_path}\")\n",
        "\n",
        "# Preprocesamiento las imágenes\n",
        "def preprocess_image(image, target_size=(64, 64)):\n",
        "    if isinstance(image, str):  # Si es una ruta, carga la imagen\n",
        "        image = cv2.imread(image)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"No se pudo leer la imagen en la ruta: {image}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, target_size)\n",
        "    return image / 255.0  # Normalizar entre 0 y 1\n",
        "\n",
        "# Cargar imágenes desde carpetas\n",
        "def load_images_from_folders(folder_paths, target_size=(64, 64)):\n",
        "    images = []\n",
        "    image_labels = []\n",
        "    class_labels = sorted(os.listdir(folder_paths))  # Nombre carpetas\n",
        "    label_map = {name: idx for idx, name in enumerate(class_labels)}\n",
        "    for label_name, label in label_map.items():\n",
        "        folder = os.path.join(folder_paths, label_name)\n",
        "        for filename in os.listdir(folder):\n",
        "            image_path = os.path.join(folder, filename)\n",
        "            if filename.endswith(('.png', '.jpg', '.jpeg')):  # Extensiones permitidas\n",
        "                try:\n",
        "                    img = preprocess_image(image_path, target_size)\n",
        "                    images.append(img)\n",
        "                    image_labels.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error cargando la imagen {image_path}: {e}\")\n",
        "    return np.array(images), np.array(image_labels), label_map\n",
        "\n",
        "# Carga imágenes y etiquetas\n",
        "print(\"Cargando imágenes desde la carpeta descomprimida...\")\n",
        "X, y, label_map = load_images_from_folders(dataset_path)\n",
        "num_classes = len(label_map)  # Total de clases\n",
        "y = to_categorical(y, num_classes=num_classes)  # Codificación one-hot\n",
        "\n",
        "# Divide los datos en entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el modelo CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')  # Clases de frutas y verduras\n",
        "])\n",
        "\n",
        "# Compilacion modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento modelo\n",
        "print(\"Entrenando el modelo...\")\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32)\n",
        "\n",
        "# Creacion carpeta para modelos\n",
        "output_path = '/content/models'\n",
        "create_folder(output_path)\n",
        "\n",
        "# Guardar el modelo en formato H5\n",
        "h5_model_path = os.path.join(output_path, 'fruit_veg_classifier.h5')\n",
        "model.save(h5_model_path)\n",
        "print(f\"Modelo guardado en: {h5_model_path}\")\n",
        "\n",
        "# Guardar como JSON y BIN\n",
        "output_path = \"./models\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "json_path = os.path.join(output_path, 'fruit_veg_classifier.json')\n",
        "model_json = model.to_json()\n",
        "with open(json_path, \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Guardar los pesos en un único archivo BIN\n",
        "bin_path = os.path.join(output_path, 'fruit_veg_classifier.bin')\n",
        "weights = model.get_weights()\n",
        "with open(bin_path, \"wb\") as bin_file:\n",
        "    for weight in weights:\n",
        "        np.save(bin_file, weight)\n",
        "print(\"Modelo guardado en los formatos H5, JSON y BIN.\")\n",
        "\n",
        "# Función para predecir el tipo\n",
        "def predict_category(image, model, target_size=(64, 64), label_map=None):\n",
        "    img = preprocess_image(image, target_size)\n",
        "    img = np.expand_dims(img, axis=0)  # Añadir dimensión para el batch\n",
        "    predictions = model.predict(img)\n",
        "    class_idx = np.argmax(predictions)\n",
        "    label = [name for name, idx in label_map.items() if idx == class_idx][0]\n",
        "    return label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XTyNiPxFK--p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de predicción\n",
        "test_image_path = '/content/dataset/Brocoli/brocoli.jpg'  # Sube imagen para probar\n",
        "if os.path.exists(test_image_path):\n",
        "    print(\"Cargando modelo para realizar predicción...\")\n",
        "    model = tf.keras.models.load_model(h5_model_path)\n",
        "    predicted_label = predict_category(test_image_path, model, label_map=label_map)\n",
        "    print(f\"Prediccion: {predicted_label}\")\n",
        "else:\n",
        "    print(f\"No se encontró la imagen de prueba en: {test_image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujXpBdIkMYQl",
        "outputId": "dbf157cf-4ebb-4fd4-c69f-bd65455d72bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo para realizar predicción...\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "Prediccion: Brocoli\n"
          ]
        }
      ]
    }
  ]
}